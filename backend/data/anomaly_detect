import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Load and clean
df = pd.read_csv("backend/data/cleaned_billing_data.csv")

# Basic cleanup
df['fresh_water_usage'] = pd.to_numeric(df['fresh_water_usage'], errors='coerce')
df['waste_water_usage'] = pd.to_numeric(df['waste_water_usage'], errors='coerce')
df['latest_charges'] = pd.to_numeric(df['latest_charges'], errors='coerce')
df['customer_id'] = df['account_number']  # ensure consistency

# Create total water usage column
df['total_water_usage'] = df['fresh_water_usage'] + df['waste_water_usage']

# Drop rows with missing core features
df.dropna(subset=['total_water_usage', 'latest_charges'], inplace=True)

# Define features
core_features = ['total_water_usage', 'latest_charges']

# Split into high and low bill count groups
bill_counts = df.groupby('customer_id').size()
high_customers = bill_counts[bill_counts >= 3].index
low_customers = bill_counts[bill_counts < 3].index

# Collect scores and indexes for high customers
high_scores = []
high_indices = []

for cust_id in high_customers:
    cust_df = df[df['customer_id'] == cust_id]
    X = cust_df[core_features]
    if len(X) < 3:
        continue
    model = IsolationForest(n_estimators=100, contamination='auto', random_state=42)
    model.fit(X)
    scores = model.decision_function(X)  # Higher scores = normal, lower = anomaly
    high_scores.extend(scores)
    high_indices.extend(cust_df.index)

high_scores_df = pd.DataFrame({'index': high_indices, 'score': high_scores})

# Prepare low customers data
df_low = df[df['customer_id'].isin(low_customers)].copy()
low_scores = []
low_indices = []

if not df_low.empty:
    scaler = StandardScaler()
    X_low = scaler.fit_transform(df_low[core_features])
    kmeans = KMeans(n_clusters=3, random_state=42)
    df_low['cluster'] = kmeans.fit_predict(X_low)

    for cluster in df_low['cluster'].unique():
        cluster_df = df_low[df_low['cluster'] == cluster]
        X_cluster = cluster_df[core_features]
        model = IsolationForest(n_estimators=100, contamination='auto', random_state=42)
        model.fit(X_cluster)
        scores = model.decision_function(X_cluster)
        low_scores.extend(scores)
        low_indices.extend(cluster_df.index)

low_scores_df = pd.DataFrame({'index': low_indices, 'score': low_scores})

# Combine all scores
all_scores_df = pd.concat([high_scores_df, low_scores_df], ignore_index=True)

# Sort ascending (lowest score = most anomalous)
all_scores_df.sort_values('score', inplace=True)

# Pick top N anomalies you want (e.g., 20)
N = 20
anomaly_indices = all_scores_df.head(N)['index']

# Flag anomalies in original df
df['anomaly_flag'] = 0
df.loc[anomaly_indices, 'anomaly_flag'] = 1

# Save outputs
df.to_csv("full_billing_anomaly_output.csv", index=False)
df[df['anomaly_flag'] == 1].to_csv("anomalies_only.csv", index=False)
